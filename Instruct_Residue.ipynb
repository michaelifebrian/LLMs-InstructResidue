{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Login to HuggingFace for downloading a gated model and uploading trained model"
      ],
      "metadata": {
        "id": "wBVughWgyfHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token your_hf_token_here"
      ],
      "metadata": {
        "id": "gH-EokHAyti_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Library"
      ],
      "metadata": {
        "id": "PdiRB9yvyx2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9y487GtyR9G"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install \"huggingface_hub[hf_transfer]\"\n",
        "!pip install git+https://github.com/huggingface/transformers.git -qq\n",
        "!pip3 install datasets\n",
        "import torch\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the Instruct Residue and plugging it back to updated base model"
      ],
      "metadata": {
        "id": "WuwvT8mjy7C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download meta-llama/Llama-3.2-3B-Instruct\n",
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download meta-llama/Llama-3.2-3B\n",
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download michaelifebrian/Llama-3.2-3B-contpretrain-Medical\n",
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B-Instruct')\n",
        "\n",
        "pretrainedmodel = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", torch_dtype=torch.bfloat16)\n",
        "continuedpretrainedmodel = transformers.AutoModelForCausalLM.from_pretrained(\"michaelifebrian/Llama-3.2-3B-contpretrain-Medical\", torch_dtype=torch.bfloat16)\n",
        "instructresiduemodel = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "# Extracting Instruct-Residue\n",
        "with torch.no_grad():\n",
        "    for instruct, base in tqdm(zip(instructresiduemodel.parameters(), pretrainedmodel.parameters())):\n",
        "        intruct.data = instruct.data - base.data\n",
        "\n",
        "# Plugging back the Instruct-Residue to updated base model\n",
        "with torch.no_grad():\n",
        "    for copr, inst in tqdm(zip(continuedpretrainedmodel.parameters(), instructresiduemodel.parameters())):\n",
        "        copr.data = copr.data + inst.data\n",
        "\n",
        "continuedpretrainedmodel.push_to_hub(\"Llama-3.2-3B-Updated-InstResi\")"
      ],
      "metadata": {
        "id": "w2s7cvyOy7nb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}